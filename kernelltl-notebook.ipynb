{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd341d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T08:56:48.581303Z",
     "iopub.status.busy": "2025-10-14T08:56:48.580625Z",
     "iopub.status.idle": "2025-10-14T08:56:49.427542Z",
     "shell.execute_reply": "2025-10-14T08:56:49.426840Z"
    },
    "papermill": {
     "duration": 0.850501,
     "end_time": "2025-10-14T08:56:49.428652",
     "exception": false,
     "start_time": "2025-10-14T08:56:48.578151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KernelLTL'...\r\n",
      "remote: Enumerating objects: 281, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (56/56), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\r\n",
      "remote: Total 281 (delta 31), reused 30 (delta 10), pack-reused 225 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (281/281), 122.98 KiB | 6.15 MiB/s, done.\r\n",
      "Resolving deltas: 100% (160/160), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/MeesLursen/KernelLTL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde0408e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T08:56:49.433321Z",
     "iopub.status.busy": "2025-10-14T08:56:49.433062Z",
     "iopub.status.idle": "2025-10-14T08:56:49.829492Z",
     "shell.execute_reply": "2025-10-14T08:56:49.828759Z"
    },
    "papermill": {
     "duration": 0.400069,
     "end_time": "2025-10-14T08:56:49.830804",
     "exception": false,
     "start_time": "2025-10-14T08:56:49.430735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!cd /kaggle/working/KernelLTL && git pull && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9b4103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T08:56:49.835316Z",
     "iopub.status.busy": "2025-10-14T08:56:49.835117Z",
     "iopub.status.idle": "2025-10-14T08:56:49.838795Z",
     "shell.execute_reply": "2025-10-14T08:56:49.838261Z"
    },
    "papermill": {
     "duration": 0.007165,
     "end_time": "2025-10-14T08:56:49.839828",
     "exception": false,
     "start_time": "2025-10-14T08:56:49.832663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/working/KernelLTL\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3199fcbd",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-14T08:56:49.844352Z",
     "iopub.status.busy": "2025-10-14T08:56:49.843725Z",
     "iopub.status.idle": "2025-10-14T08:59:49.271481Z",
     "shell.execute_reply": "2025-10-14T08:59:49.270329Z"
    },
    "papermill": {
     "duration": 179.431624,
     "end_time": "2025-10-14T08:59:49.273191",
     "exception": false,
     "start_time": "2025-10-14T08:56:49.841567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin/micromamba\r\n",
      "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\r\n",
      "conda-forge/linux-64 \u001b[90m━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\r\n",
      "conda-forge/noarch   \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\r\n",
      "conda-forge/linux-64 ╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   2.7MB /  47.8MB @  17.2MB/s  0.1s\r\n",
      "conda-forge/noarch   ╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   2.0MB /  22.8MB @  12.5MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\r\n",
      "conda-forge/linux-64 ━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m   7.6MB /  47.8MB @  29.4MB/s  0.2s\r\n",
      "conda-forge/noarch   ━━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m   6.9MB /  22.8MB @  26.3MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\r\n",
      "conda-forge/linux-64 ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m  12.5MB /  47.8MB @  34.7MB/s  0.3s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━\u001b[0m  11.9MB /  22.8MB @  32.4MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\r\n",
      "conda-forge/linux-64 ━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━━\u001b[0m  17.6MB /  47.8MB @  37.6MB/s  0.4s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━\u001b[0m  16.8MB /  22.8MB @  35.7MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━\u001b[0m  22.6MB /  47.8MB @  39.8MB/s  0.5s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  21.9MB /  22.8MB @  38.2MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                  22.8MB @  38.2MB/s  0.6s\r\n",
      "[+] 0.7s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━\u001b[0m  24.0MB /  47.8MB @  34.8MB/s  0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━\u001b[0m  33.2MB /  47.8MB @  41.9MB/s  0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m  42.9MB /  47.8MB @  48.0MB/s  0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m  42.9MB /  47.8MB @  48.0MB/s  0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m  42.9MB /  47.8MB @  48.0MB/s  1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━\u001b[0m  42.9MB /  47.8MB @  48.0MB/s  1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                47.8MB @  48.0MB/s  1.2s\r\n",
      "\u001b[?25h\r\n",
      "\r\n",
      "Transaction\r\n",
      "\r\n",
      "  Prefix: /root/.local/share/mamba/envs/kernelltl\r\n",
      "\r\n",
      "  Updating specs:\r\n",
      "\r\n",
      "   - python=3.13.7\r\n",
      "   - pip\r\n",
      "\r\n",
      "\r\n",
      "  Package               Version  Build               Channel          Size\r\n",
      "────────────────────────────────────────────────────────────────────────────\r\n",
      "  Install:\r\n",
      "────────────────────────────────────────────────────────────────────────────\r\n",
      "\r\n",
      "  \u001b[32m+ _libgcc_mutex   \u001b[0m        0.1  conda_forge         conda-forge       3kB\r\n",
      "  \u001b[32m+ _openmp_mutex   \u001b[0m        4.5  2_gnu               conda-forge      24kB\r\n",
      "  \u001b[32m+ bzip2           \u001b[0m      1.0.8  hda65f42_8          conda-forge     260kB\r\n",
      "  \u001b[32m+ ca-certificates \u001b[0m  2025.10.5  hbd8a1cb_0          conda-forge     156kB\r\n",
      "  \u001b[32m+ ld_impl_linux-64\u001b[0m       2.44  ha97dd6f_2          conda-forge     747kB\r\n",
      "  \u001b[32m+ libexpat        \u001b[0m      2.7.1  hecca717_0          conda-forge      75kB\r\n",
      "  \u001b[32m+ libffi          \u001b[0m      3.4.6  h2dba641_1          conda-forge      57kB\r\n",
      "  \u001b[32m+ libgcc          \u001b[0m     15.2.0  h767d61c_7          conda-forge     823kB\r\n",
      "  \u001b[32m+ libgomp         \u001b[0m     15.2.0  h767d61c_7          conda-forge     448kB\r\n",
      "  \u001b[32m+ liblzma         \u001b[0m      5.8.1  hb9d3cd8_2          conda-forge     113kB\r\n",
      "  \u001b[32m+ libmpdec        \u001b[0m      4.0.0  hb9d3cd8_0          conda-forge      91kB\r\n",
      "  \u001b[32m+ libsqlite       \u001b[0m     3.50.4  h0c1763c_0          conda-forge     933kB\r\n",
      "  \u001b[32m+ libuuid         \u001b[0m     2.41.2  he9a06e4_0          conda-forge      37kB\r\n",
      "  \u001b[32m+ libzlib         \u001b[0m      1.3.1  hb9d3cd8_2          conda-forge      61kB\r\n",
      "  \u001b[32m+ ncurses         \u001b[0m        6.5  h2d0b736_3          conda-forge     892kB\r\n",
      "  \u001b[32m+ openssl         \u001b[0m      3.5.4  h26f9b46_0          conda-forge       3MB\r\n",
      "  \u001b[32m+ pip             \u001b[0m       25.2  pyh145f28c_0        conda-forge       1MB\r\n",
      "  \u001b[32m+ python          \u001b[0m     3.13.7  h2b335a9_100_cp313  conda-forge      34MB\r\n",
      "  \u001b[32m+ python_abi      \u001b[0m       3.13  8_cp313             conda-forge       7kB\r\n",
      "  \u001b[32m+ readline        \u001b[0m        8.2  h8c095d6_2          conda-forge     282kB\r\n",
      "  \u001b[32m+ tk              \u001b[0m     8.6.13  noxft_hd72426e_102  conda-forge       3MB\r\n",
      "  \u001b[32m+ tzdata          \u001b[0m      2025b  h78e105d_0          conda-forge     123kB\r\n",
      "\r\n",
      "  Summary:\r\n",
      "\r\n",
      "  Install: 22 packages\r\n",
      "\r\n",
      "  Total download: 46MB\r\n",
      "\r\n",
      "────────────────────────────────────────────────────────────────────────────\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Transaction starting\r\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\r\n",
      "Downloading      ╸\u001b[33m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B                            0.0s\r\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpip                                                  1.2MB @  ??.?MB/s  0.0s\r\n",
      "[+] 0.1s\r\n",
      "Downloading  (5) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   1.2MB libsqlite                  0.0s\r\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibsqlite                                          932.6kB @   1.7MB/s  0.1s\r\n",
      "tk                                                   3.3MB @   9.4MB/s  0.1s\r\n",
      "ncurses                                            891.6kB @  17.8MB/s  0.1s\r\n",
      "libgcc                                             822.6kB @  ??.?MB/s  0.0s\r\n",
      "openssl                                              3.1MB @  14.9MB/s  0.1s\r\n",
      "ld_impl_linux-64                                   747.2kB @   3.1MB/s  0.1s\r\n",
      "libgomp                                            447.9kB @  ??.?MB/s  0.0s\r\n",
      "[+] 0.2s\r\n",
      "Downloading  (5) ━━━━━━━━━━━╸\u001b[33m━━━━━━━━━━━\u001b[0m  24.6MB bzip2                      0.1s\r\n",
      "Extracting   (6) ╸\u001b[33m━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m       1 ld_impl_linux-64           0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Greadline                                           282.5kB @  ??.?MB/s  0.0s\r\n",
      "bzip2                                              260.3kB @  ??.?MB/s  0.0s\r\n",
      "ca-certificates                                    155.9kB @  ??.?MB/s  0.0s\r\n",
      "tzdata                                             123.0kB @  ??.?MB/s  0.0s\r\n",
      "liblzma                                            112.9kB @  ??.?MB/s  0.0s\r\n",
      "libexpat                                            74.8kB @  ??.?MB/s  0.0s\r\n",
      "libmpdec                                            91.2kB @  ??.?MB/s  0.1s\r\n",
      "libzlib                                             61.0kB @  ??.?MB/s  0.1s\r\n",
      "libffi                                              57.4kB @  ??.?MB/s  0.0s\r\n",
      "python                                              33.6MB @  93.3MB/s  0.2s\r\n",
      "[+] 0.3s\r\n",
      "Downloading  (3) ━━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━\u001b[0m  46.3MB _libgcc_mutex              0.2s\r\n",
      "Extracting  (14) ━━╸\u001b[33m━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m       3 ld_impl_linux-64           0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibuuid                                             37.1kB @  ??.?MB/s  0.0s\r\n",
      "_openmp_mutex                                       23.6kB @  ??.?MB/s  0.0s\r\n",
      "python_abi                                           7.0kB @  ??.?MB/s  0.0s\r\n",
      "_libgcc_mutex                                        2.6kB @  ??.?MB/s  0.0s\r\n",
      "[+] 0.4s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting  (12) ━━━━━━━━╸\u001b[33m━━━━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m       9 _libgcc_mutex              0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting   (7) ━━━━━━━━━━━━━━╸\u001b[33m━━━━━━━━\u001b[0m      15 _openmp_mutex              0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting   (3) ━━━━━━━━━━━━━━━━━━╸\u001b[33m━━━━\u001b[0m      19 ncurses                    0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25hLinking ld_impl_linux-64-2.44-ha97dd6f_2\r\n",
      "Linking libgomp-15.2.0-h767d61c_7\r\n",
      "Linking _libgcc_mutex-0.1-conda_forge\r\n",
      "Linking _openmp_mutex-4.5-2_gnu\r\n",
      "Linking libgcc-15.2.0-h767d61c_7\r\n",
      "Linking libmpdec-4.0.0-hb9d3cd8_0\r\n",
      "Linking ncurses-6.5-h2d0b736_3\r\n",
      "Linking libzlib-1.3.1-hb9d3cd8_2\r\n",
      "Linking libuuid-2.41.2-he9a06e4_0\r\n",
      "Linking liblzma-5.8.1-hb9d3cd8_2\r\n",
      "Linking libffi-3.4.6-h2dba641_1\r\n",
      "Linking bzip2-1.0.8-hda65f42_8\r\n",
      "Linking libexpat-2.7.1-hecca717_0\r\n",
      "Linking readline-8.2-h8c095d6_2\r\n",
      "Linking libsqlite-3.50.4-h0c1763c_0\r\n",
      "Linking tk-8.6.13-noxft_hd72426e_102\r\n",
      "Linking python_abi-3.13-8_cp313\r\n",
      "Linking tzdata-2025b-h78e105d_0\r\n",
      "Linking ca-certificates-2025.10.5-hbd8a1cb_0\r\n",
      "Linking openssl-3.5.4-h26f9b46_0\r\n",
      "Linking python-3.13.7-h2b335a9_100_cp313\r\n",
      "Linking pip-25.2-pyh145f28c_0\r\n",
      "\r\n",
      "Transaction finished\r\n",
      "\r\n",
      "\r\n",
      "To activate this environment, use:\r\n",
      "\r\n",
      "    micromamba activate kernelltl\r\n",
      "\r\n",
      "Or to execute a single command in this environment, use:\r\n",
      "\r\n",
      "    micromamba run -n kernelltl mycommand\r\n",
      "\r\n",
      "Collecting accelerate==1.10.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 1))\r\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting certifi==2025.10.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 2))\r\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting charset-normalizer==3.4.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 3))\r\n",
      "  Downloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\r\n",
      "Collecting filelock==3.20.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 4))\r\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting fsspec==2025.9.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 5))\r\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting hf-xet==1.1.10 (from -r /kaggle/working/KernelLTL/requirements.txt (line 6))\r\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\r\n",
      "Collecting huggingface-hub==0.35.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 7))\r\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting idna==3.10 (from -r /kaggle/working/KernelLTL/requirements.txt (line 8))\r\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting Jinja2==3.1.6 (from -r /kaggle/working/KernelLTL/requirements.txt (line 9))\r\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting MarkupSafe==3.0.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 10))\r\n",
      "  Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\r\n",
      "Collecting mpmath==1.3.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 11))\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Collecting networkx==3.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 12))\r\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting numpy==2.3.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 13))\r\n",
      "  Downloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 14))\r\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 15))\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 16))\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 17))\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from -r /kaggle/working/KernelLTL/requirements.txt (line 18))\r\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from -r /kaggle/working/KernelLTL/requirements.txt (line 19))\r\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 20))\r\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 21))\r\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 22))\r\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 23))\r\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 24))\r\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 25))\r\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 26))\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 27))\r\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting packaging==25.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 28))\r\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting psutil==7.1.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 29))\r\n",
      "  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\r\n",
      "Collecting PyYAML==6.0.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 30))\r\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\r\n",
      "Collecting regex==2025.9.18 (from -r /kaggle/working/KernelLTL/requirements.txt (line 31))\r\n",
      "  Downloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\r\n",
      "Collecting requests==2.32.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 32))\r\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting safetensors==0.6.2 (from -r /kaggle/working/KernelLTL/requirements.txt (line 33))\r\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n",
      "Collecting setuptools==80.9.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 34))\r\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\r\n",
      "Collecting sympy==1.14.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 35))\r\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting tokenizers==0.22.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 36))\r\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting torch==2.8.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 37))\r\n",
      "  Downloading torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\r\n",
      "Collecting tqdm==4.67.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 38))\r\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting transformers==4.57.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 39))\r\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\r\n",
      "Collecting triton==3.4.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 40))\r\n",
      "  Downloading triton-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting typing_extensions==4.15.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 41))\r\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting urllib3==2.5.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 42))\r\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\r\n",
      "Downloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m135.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading certifi-2025.10.5-py3-none-any.whl (163 kB)\r\n",
      "Downloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\r\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\r\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\r\n",
      "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\r\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\r\n",
      "Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\r\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m125.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\r\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\r\n",
      "Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\r\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.1/802.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\r\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\r\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\r\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl (887.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0m\r\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\r\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing_extensions, tqdm, sympy, setuptools, safetensors, regex, PyYAML, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, Jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/42\u001b[0m [accelerate]\r\n",
      "\u001b[1A\u001b[2KSuccessfully installed Jinja2-3.1.6 MarkupSafe-3.0.3 PyYAML-6.0.3 accelerate-1.10.1 certifi-2025.10.5 charset-normalizer-3.4.3 filelock-3.20.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.3 idna-3.10 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 psutil-7.1.0 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 tqdm-4.67.1 transformers-4.57.0 triton-3.4.0 typing_extensions-4.15.0 urllib3-2.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!wget -qO- https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n",
    "!./bin/micromamba create -y -n kernelltl -c conda-forge python=3.13.7 pip\n",
    "!./bin/micromamba run -n kernelltl pip install -r /kaggle/working/KernelLTL/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c80de71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T08:59:49.348909Z",
     "iopub.status.busy": "2025-10-14T08:59:49.348630Z",
     "iopub.status.idle": "2025-10-14T09:29:30.400290Z",
     "shell.execute_reply": "2025-10-14T09:29:30.399361Z"
    },
    "papermill": {
     "duration": 1781.09027,
     "end_time": "2025-10-14T09:29:30.401908",
     "exception": false,
     "start_time": "2025-10-14T08:59:49.311638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1014 08:59:51.460000 198 site-packages/torch/distributed/run.py:774] \r\n",
      "W1014 08:59:51.460000 198 site-packages/torch/distributed/run.py:774] *****************************************\r\n",
      "W1014 08:59:51.460000 198 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W1014 08:59:51.460000 198 site-packages/torch/distributed/run.py:774] *****************************************\r\n",
      "Finished building Kernel.\r\n",
      "Finished building Kernel.\r\n",
      "Finished train and eval dataset construction.\r\n",
      "Finished train and eval dataset construction.\r\n",
      "Started training.\r\n",
      "Started training.\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:54: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  ALLREDUCE = partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:54: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  ALLREDUCE = partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:55: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  FP16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:58: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BF16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:55: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  FP16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:61: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_TENSOR = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:58: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BF16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:64: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_CHANNEL = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:67: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:61: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_TENSOR = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:74: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:64: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_CHANNEL = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:80: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:85: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:67: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:90: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  NOOP = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:74: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:80: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:85: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:90: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  NOOP = partial(\r\n",
      "  0%|                                                 | 0/12190 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "{'loss': 2.6441, 'grad_norm': 4.601202011108398, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.08}\r\n",
      "{'loss': 2.6236, 'grad_norm': 2.339883804321289, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.16}\r\n",
      "{'loss': 2.6153, 'grad_norm': 2.2838287353515625, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.25}\r\n",
      "{'loss': 2.5762, 'grad_norm': 1.6394374370574951, 'learning_rate': 3.99e-05, 'epoch': 0.33}\r\n",
      "{'loss': 2.5339, 'grad_norm': 2.1938507556915283, 'learning_rate': 4.99e-05, 'epoch': 0.41}\r\n",
      "{'loss': 2.4188, 'grad_norm': 1.4548736810684204, 'learning_rate': 4.9576561163387515e-05, 'epoch': 0.49}\r\n",
      "{'loss': 2.1425, 'grad_norm': 2.979966878890991, 'learning_rate': 4.9148845166809243e-05, 'epoch': 0.57}\r\n",
      "{'loss': 1.9674, 'grad_norm': 3.563542366027832, 'learning_rate': 4.8721129170230965e-05, 'epoch': 0.66}\r\n",
      "{'loss': 1.7658, 'grad_norm': 6.844625473022461, 'learning_rate': 4.8293413173652694e-05, 'epoch': 0.74}\r\n",
      "{'loss': 1.5229, 'grad_norm': 7.917372226715088, 'learning_rate': 4.786569717707442e-05, 'epoch': 0.82}\r\n",
      "{'loss': 1.2804, 'grad_norm': 7.628044605255127, 'learning_rate': 4.743798118049615e-05, 'epoch': 0.9}\r\n",
      "{'loss': 1.1228, 'grad_norm': 3.3017547130584717, 'learning_rate': 4.701026518391788e-05, 'epoch': 0.98}\r\n",
      " 10%|███▌                                | 1219/12190 [14:41<2:25:18,  1.26it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\r\n",
      "\r\n",
      "  Epoch 1.0:\r\n",
      "  Semantic Distance: 1.0000\r\n",
      "  Exact Match Rate: 0.0000\r\n",
      "[rank0]: Traceback (most recent call last):\r\n",
      "[rank0]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n",
      "[rank0]:   File \"<frozen runpy>\", line 88, in _run_code\r\n",
      "[rank0]:   File \"/kaggle/working/KernelLTL/train.py\", line 134, in <module>\r\n",
      "[rank0]:     main()\r\n",
      "[rank0]:     ~~~~^^\r\n",
      "[rank0]:   File \"/kaggle/working/KernelLTL/train.py\", line 127, in main\r\n",
      "[rank0]:     trainer.train()\r\n",
      "[rank0]:     ~~~~~~~~~~~~~^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 2325, in train\r\n",
      "[rank0]:     return inner_training_loop(\r\n",
      "[rank0]:         args=args,\r\n",
      "[rank0]:     ...<2 lines>...\r\n",
      "[rank0]:         ignore_keys_for_eval=ignore_keys_for_eval,\r\n",
      "[rank0]:     )\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 2789, in _inner_training_loop\r\n",
      "[rank0]:     self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\r\n",
      "[rank0]:                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer_callback.py\", line 516, in on_epoch_end\r\n",
      "[rank0]:     return self.call_event(\"on_epoch_end\", args, state, control)\r\n",
      "[rank0]:            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer_callback.py\", line 556, in call_event\r\n",
      "[rank0]:     result = getattr(callback, event)(\r\n",
      "[rank0]:         args,\r\n",
      "[rank0]:     ...<8 lines>...\r\n",
      "[rank0]:         **kwargs,\r\n",
      "[rank0]:     )\r\n",
      "[rank0]:   File \"/kaggle/working/KernelLTL/training_utils.py\", line 124, in on_epoch_end\r\n",
      "[rank0]:     print(f\"  Exact Match Strings: {exact_matches_strs:.4f}\")\r\n",
      "[rank0]:                                    ^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]: TypeError: unsupported format string passed to list.__format__\r\n",
      " 10%|███▌                                | 1219/12190 [22:40<3:24:00,  1.12s/it]\r\n",
      "[rank0]:[W1014 09:29:28.106573358 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n",
      "W1014 09:29:29.814000 198 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 211 closing signal SIGTERM\r\n",
      "E1014 09:29:29.933000 198 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 210) of binary: /root/.local/share/mamba/envs/kernelltl/bin/python3.13\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/bin/torchrun\"\u001b[0m, line \u001b[35m7\u001b[0m, in \u001b[35m<module>\u001b[0m\r\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\r\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m357\u001b[0m, in \u001b[35mwrapper\u001b[0m\r\n",
      "    return f(*args, **kwargs)\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m901\u001b[0m, in \u001b[35mmain\u001b[0m\r\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\r\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mrun\u001b[0m\r\n",
      "    \u001b[31melastic_launch(\u001b[0m\r\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\r\n",
      "        \u001b[31mconfig=config,\u001b[0m\r\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\r\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\r\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\r\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\r\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35m__call__\u001b[0m\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m277\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\r\n",
      "    raise ChildFailedError(\r\n",
      "    ...<2 lines>...\r\n",
      "    )\r\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\r\n",
      "============================================================\r\n",
      "train FAILED\r\n",
      "------------------------------------------------------------\r\n",
      "Failures:\r\n",
      "  <NO_OTHER_FAILURES>\r\n",
      "------------------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2025-10-14_09:29:29\r\n",
      "  host      : 0311773990c7\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : 1 (pid: 210)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\r\n",
      "============================================================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!cd /kaggle/working/KernelLTL && \\\n",
    " PYTHONPATH=$PWD /kaggle/working/bin/micromamba run -n kernelltl \\\n",
    " torchrun --nproc_per_node=2 -m train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0b9527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T09:29:30.567517Z",
     "iopub.status.busy": "2025-10-14T09:29:30.567265Z",
     "iopub.status.idle": "2025-10-14T09:29:31.276030Z",
     "shell.execute_reply": "2025-10-14T09:29:31.275394Z"
    },
    "papermill": {
     "duration": 0.791197,
     "end_time": "2025-10-14T09:29:31.277224",
     "exception": false,
     "start_time": "2025-10-14T09:29:30.486027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ref\": \"\", \"url\": \"https://www.kaggle.com/\", \"status\": \"Error\", \"error\": \"Please upload at least one file\", \"invalidTags\": []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, shutil\n",
    "\n",
    "import os, shutil\n",
    "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "shutil.copy(\"/kaggle/input/kaggle-api-token/kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "\n",
    "from kaggle import api\n",
    "ds_dir = \"/kaggle/working/KernelLTL/ltl_model_outputs\"\n",
    "os.makedirs(ds_dir, exist_ok=True)\n",
    "\n",
    "metadata = {\n",
    "    \"title\": \"LTL model output\",\n",
    "    \"id\": \"tayhnd5sgdtjevyreuw/kernel-ltl-model-outputs\",\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "}\n",
    "with open(os.path.join(ds_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "api.dataset_create_new(ds_dir)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8481667,
     "sourceId": 13369742,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1967.069205,
   "end_time": "2025-10-14T09:29:31.679326",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-14T08:56:44.610121",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

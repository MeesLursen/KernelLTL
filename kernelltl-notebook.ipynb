{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/MeesLursen/KernelLTL.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:09:31.967505Z","iopub.execute_input":"2025-10-12T18:09:31.967778Z","iopub.status.idle":"2025-10-12T18:09:32.085298Z","shell.execute_reply.started":"2025-10-12T18:09:31.967753Z","shell.execute_reply":"2025-10-12T18:09:32.084693Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'KernelLTL' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!cd /kaggle/working/KernelLTL && git pull && cd ..","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:50:12.165058Z","iopub.execute_input":"2025-10-12T18:50:12.165426Z","iopub.status.idle":"2025-10-12T18:50:13.102726Z","shell.execute_reply.started":"2025-10-12T18:50:12.165390Z","shell.execute_reply":"2025-10-12T18:50:13.101872Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (1/1), done.\u001b[K\nremote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (3/3), 267 bytes | 267.00 KiB/s, done.\nFrom https://github.com/MeesLursen/KernelLTL\n   06f9ebf..f8bb618  main       -> origin/main\nUpdating 06f9ebf..f8bb618\nFast-forward\n train.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 1 insertion(+), 1 deletion(-)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import sys\nsys.path.insert(0, \"/kaggle/working/KernelLTL\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:09:36.820038Z","iopub.execute_input":"2025-10-12T18:09:36.820681Z","iopub.status.idle":"2025-10-12T18:09:36.824444Z","shell.execute_reply.started":"2025-10-12T18:09:36.820628Z","shell.execute_reply":"2025-10-12T18:09:36.823773Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"!wget -qO- https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n!./bin/micromamba create -y -n kernelltl -c conda-forge python=3.13.7 pip\n!./bin/micromamba run -n kernelltl pip install -r /kaggle/working/KernelLTL/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:09:37.644728Z","iopub.execute_input":"2025-10-12T18:09:37.645405Z","iopub.status.idle":"2025-10-12T18:13:00.049957Z","shell.execute_reply.started":"2025-10-12T18:09:37.645378Z","shell.execute_reply":"2025-10-12T18:13:00.049023Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"bin/micromamba\n\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nconda-forge/linux-64 \u001b[90m━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\nconda-forge/noarch   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m  14.1kB /  22.8MB @ 193.0kB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\nconda-forge/linux-64 ╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4.1MB /  47.8MB @  23.3MB/s  0.1s\nconda-forge/noarch   ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m   4.3MB /  22.8MB @  24.3MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\nconda-forge/linux-64 ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m   9.0MB /  47.8MB @  32.0MB/s  0.2s\nconda-forge/noarch   ━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━\u001b[0m   9.2MB /  22.8MB @  32.4MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\nconda-forge/linux-64 ━━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m  13.8MB /  47.8MB @  35.8MB/s  0.3s\nconda-forge/noarch   ━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━\u001b[0m  13.9MB /  22.8MB @  36.0MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\nconda-forge/linux-64 ━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━━\u001b[0m  18.7MB /  47.8MB @  38.2MB/s  0.4s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━\u001b[0m  18.9MB /  22.8MB @  38.3MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\nconda-forge/linux-64 ━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━\u001b[0m  21.2MB /  47.8MB @  39.0MB/s  0.5s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  21.4MB /  22.8MB @  39.3MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                  22.8MB @  39.3MB/s  0.6s\n[+] 0.7s\nconda-forge/linux-64 ━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━\u001b[0m  22.8MB /  47.8MB @  33.4MB/s  0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\nconda-forge/linux-64 ━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━\u001b[0m  31.6MB /  47.8MB @  40.4MB/s  0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━\u001b[0m  40.5MB /  47.8MB @  45.7MB/s  0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  45.1MB /  47.8MB @  48.1MB/s  0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  45.1MB /  47.8MB @  48.1MB/s  1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  45.1MB /  47.8MB @  48.1MB/s  1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  45.1MB /  47.8MB @  48.1MB/s  1.2s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                47.8MB @  48.1MB/s  1.3s\n\u001b[?25h\n\nTransaction\n\n  Prefix: /root/.local/share/mamba/envs/kernelltl\n\n  Updating specs:\n\n   - python=3.13.7\n   - pip\n\n\n  Package               Version  Build               Channel          Size\n────────────────────────────────────────────────────────────────────────────\n  Install:\n────────────────────────────────────────────────────────────────────────────\n\n  \u001b[32m+ _libgcc_mutex   \u001b[0m        0.1  conda_forge         conda-forge       3kB\n  \u001b[32m+ _openmp_mutex   \u001b[0m        4.5  2_gnu               conda-forge      24kB\n  \u001b[32m+ bzip2           \u001b[0m      1.0.8  hda65f42_8          conda-forge     260kB\n  \u001b[32m+ ca-certificates \u001b[0m  2025.10.5  hbd8a1cb_0          conda-forge     156kB\n  \u001b[32m+ ld_impl_linux-64\u001b[0m       2.44  ha97dd6f_2          conda-forge     747kB\n  \u001b[32m+ libexpat        \u001b[0m      2.7.1  hecca717_0          conda-forge      75kB\n  \u001b[32m+ libffi          \u001b[0m      3.4.6  h2dba641_1          conda-forge      57kB\n  \u001b[32m+ libgcc          \u001b[0m     15.2.0  h767d61c_7          conda-forge     823kB\n  \u001b[32m+ libgomp         \u001b[0m     15.2.0  h767d61c_7          conda-forge     448kB\n  \u001b[32m+ liblzma         \u001b[0m      5.8.1  hb9d3cd8_2          conda-forge     113kB\n  \u001b[32m+ libmpdec        \u001b[0m      4.0.0  hb9d3cd8_0          conda-forge      91kB\n  \u001b[32m+ libsqlite       \u001b[0m     3.50.4  h0c1763c_0          conda-forge     933kB\n  \u001b[32m+ libuuid         \u001b[0m     2.41.2  he9a06e4_0          conda-forge      37kB\n  \u001b[32m+ libzlib         \u001b[0m      1.3.1  hb9d3cd8_2          conda-forge      61kB\n  \u001b[32m+ ncurses         \u001b[0m        6.5  h2d0b736_3          conda-forge     892kB\n  \u001b[32m+ openssl         \u001b[0m      3.5.4  h26f9b46_0          conda-forge       3MB\n  \u001b[32m+ pip             \u001b[0m       25.2  pyh145f28c_0        conda-forge       1MB\n  \u001b[32m+ python          \u001b[0m     3.13.7  h2b335a9_100_cp313  conda-forge      34MB\n  \u001b[32m+ python_abi      \u001b[0m       3.13  8_cp313             conda-forge       7kB\n  \u001b[32m+ readline        \u001b[0m        8.2  h8c095d6_2          conda-forge     282kB\n  \u001b[32m+ tk              \u001b[0m     8.6.13  noxft_hd72426e_102  conda-forge       3MB\n  \u001b[32m+ tzdata          \u001b[0m      2025b  h78e105d_0          conda-forge     123kB\n\n  Summary:\n\n  Install: 22 packages\n\n  Total download: 46MB\n\n────────────────────────────────────────────────────────────────────────────\n\n\n\nTransaction starting\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\nDownloading      ╸\u001b[33m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B                            0.0s\nExtracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibsqlite                                          932.6kB @  ??.?MB/s  0.0s\npip                                                  1.2MB @  ??.?MB/s  0.0s\n[+] 0.1s\nDownloading  (4) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   2.1MB ncurses                    0.0s\nExtracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gtk                                                   3.3MB @  21.9MB/s  0.1s\nopenssl                                              3.1MB @   2.0MB/s  0.1s\nlibgcc                                             822.6kB @  14.4MB/s  0.1s\nld_impl_linux-64                                   747.2kB @   6.6MB/s  0.1s\nlibgomp                                            447.9kB @  ??.?MB/s  0.0s\nncurses                                            891.6kB @  ??.?MB/s  0.1s\n[+] 0.2s\nDownloading  (4) ━━━━━━━╸\u001b[33m━━━━━━━━━━━━━━━\u001b[0m  17.2MB bzip2                      0.1s\nExtracting   (7) ╸\u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m       1 ld_impl_linux-64           0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Greadline                                           282.5kB @  ??.?MB/s  0.0s\nbzip2                                              260.3kB @  ??.?MB/s  0.0s\nca-certificates                                    155.9kB @  ??.?MB/s  0.0s\ntzdata                                             123.0kB @  ??.?MB/s  0.0s\nliblzma                                            112.9kB @   2.2MB/s  0.1s\nlibmpdec                                            91.2kB @  ??.?MB/s  0.0s\nlibzlib                                             61.0kB @  ??.?MB/s  0.0s\nlibexpat                                            74.8kB @  ??.?MB/s  0.0s\npython                                              33.6MB @  91.2MB/s  0.2s\n[+] 0.3s\nDownloading  (5) ━━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━\u001b[0m  46.2MB _libgcc_mutex              0.2s\nExtracting  (12) ━━━╸\u001b[33m━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m       4 ld_impl_linux-64           0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibffi                                              57.4kB @  ??.?MB/s  0.0s\nlibuuid                                             37.1kB @  ??.?MB/s  0.0s\n_openmp_mutex                                       23.6kB @  ??.?MB/s  0.0s\npython_abi                                           7.0kB @  ??.?MB/s  0.0s\n_libgcc_mutex                                        2.6kB @  35.4kB/s  0.1s\n[+] 0.4s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\nExtracting   (9) ━━━━━━━━━━━╸\u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m      12 _libgcc_mutex              0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\nExtracting   (2) ━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━━\u001b[0m      20 ncurses                    0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\nExtracting   (2) ━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━━\u001b[0m      20 ncurses                    0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\nExtracting   (2) ━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━━\u001b[0m      20 ncurses                    0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\nExtracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\nExtracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.3s\nExtracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25hLinking ld_impl_linux-64-2.44-ha97dd6f_2\nLinking libgomp-15.2.0-h767d61c_7\nLinking _libgcc_mutex-0.1-conda_forge\nLinking _openmp_mutex-4.5-2_gnu\nLinking libgcc-15.2.0-h767d61c_7\nLinking libmpdec-4.0.0-hb9d3cd8_0\nLinking ncurses-6.5-h2d0b736_3\nLinking libzlib-1.3.1-hb9d3cd8_2\nLinking libuuid-2.41.2-he9a06e4_0\nLinking liblzma-5.8.1-hb9d3cd8_2\nLinking libffi-3.4.6-h2dba641_1\nLinking bzip2-1.0.8-hda65f42_8\nLinking libexpat-2.7.1-hecca717_0\nLinking readline-8.2-h8c095d6_2\nLinking libsqlite-3.50.4-h0c1763c_0\nLinking tk-8.6.13-noxft_hd72426e_102\nLinking python_abi-3.13-8_cp313\nLinking tzdata-2025b-h78e105d_0\nLinking ca-certificates-2025.10.5-hbd8a1cb_0\nLinking openssl-3.5.4-h26f9b46_0\nLinking python-3.13.7-h2b335a9_100_cp313\nLinking pip-25.2-pyh145f28c_0\n\nTransaction finished\n\n\nTo activate this environment, use:\n\n    micromamba activate kernelltl\n\nOr to execute a single command in this environment, use:\n\n    micromamba run -n kernelltl mycommand\n\nCollecting accelerate==1.10.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 1))\n  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\nCollecting certifi==2025.10.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 2))\n  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\nCollecting charset-normalizer==3.4.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 3))\n  Downloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\nCollecting filelock==3.20.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 4))\n  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\nCollecting fsspec==2025.9.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 5))\n  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\nCollecting hf-xet==1.1.10 (from -r /kaggle/working/KernelLTL/requirements.txt (line 6))\n  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\nCollecting huggingface-hub==0.35.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 7))\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nCollecting idna==3.10 (from -r /kaggle/working/KernelLTL/requirements.txt (line 8))\n  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\nCollecting Jinja2==3.1.6 (from -r /kaggle/working/KernelLTL/requirements.txt (line 9))\n  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\nCollecting MarkupSafe==3.0.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 10))\n  Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\nCollecting mpmath==1.3.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 11))\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nCollecting networkx==3.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 12))\n  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\nCollecting numpy==2.3.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 13))\n  Downloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\nCollecting nvidia-cublas-cu12==12.8.4.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 14))\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 15))\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 16))\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 17))\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cudnn-cu12==9.10.2.21 (from -r /kaggle/working/KernelLTL/requirements.txt (line 18))\n  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from -r /kaggle/working/KernelLTL/requirements.txt (line 19))\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 20))\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 21))\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 22))\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12==12.5.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 23))\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparselt-cu12==0.7.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 24))\n  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\nCollecting nvidia-nccl-cu12==2.27.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 25))\n  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 26))\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvtx-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 27))\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting packaging==25.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 28))\n  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting psutil==7.1.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 29))\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\nCollecting PyYAML==6.0.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 30))\n  Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\nCollecting regex==2025.9.18 (from -r /kaggle/working/KernelLTL/requirements.txt (line 31))\n  Downloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nCollecting requests==2.32.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 32))\n  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\nCollecting safetensors==0.6.2 (from -r /kaggle/working/KernelLTL/requirements.txt (line 33))\n  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nCollecting setuptools==80.9.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 34))\n  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\nCollecting sympy==1.14.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 35))\n  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nCollecting tokenizers==0.22.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 36))\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting torch==2.8.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 37))\n  Downloading torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\nCollecting tqdm==4.67.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 38))\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\nCollecting transformers==4.57.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 39))\n  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\nCollecting triton==3.4.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 40))\n  Downloading triton-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nCollecting typing_extensions==4.15.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 41))\n  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting urllib3==2.5.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 42))\n  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\nDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\nDownloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n\u001b[?25hDownloading certifi-2025.10.5-py3-none-any.whl (163 kB)\nDownloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\nDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\nDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\nDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\nDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\nDownloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\nDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m881.7 kB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\nDownloading packaging-25.0-py3-none-any.whl (66 kB)\nDownloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\nDownloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.1/802.1 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\nDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\nDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\nDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl (887.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m  \u001b[33m0:00:17\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\nDownloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m36m0:00:01\u001b[0m\n\u001b[?25hDownloading triton-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\nInstalling collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing_extensions, tqdm, sympy, setuptools, safetensors, regex, PyYAML, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, Jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/42\u001b[0m [accelerate]celerate]ansformers]ub]cu12]2]2]\n\u001b[1A\u001b[2KSuccessfully installed Jinja2-3.1.6 MarkupSafe-3.0.3 PyYAML-6.0.3 accelerate-1.10.1 certifi-2025.10.5 charset-normalizer-3.4.3 filelock-3.20.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.3 idna-3.10 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 psutil-7.1.0 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 tqdm-4.67.1 transformers-4.57.0 triton-3.4.0 typing_extensions-4.15.0 urllib3-2.5.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!cd /kaggle/working/KernelLTL && \\\n PYTHONPATH=$PWD /kaggle/working/bin/micromamba run -n kernelltl \\\n torchrun --nproc_per_node=2 -m train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T18:50:29.307886Z","iopub.execute_input":"2025-10-12T18:50:29.308577Z","iopub.status.idle":"2025-10-12T19:11:38.815427Z","shell.execute_reply.started":"2025-10-12T18:50:29.308545Z","shell.execute_reply":"2025-10-12T19:11:38.814517Z"}},"outputs":[{"name":"stdout","text":"W1012 18:50:30.954000 424 site-packages/torch/distributed/run.py:774] \nW1012 18:50:30.954000 424 site-packages/torch/distributed/run.py:774] *****************************************\nW1012 18:50:30.954000 424 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1012 18:50:30.954000 424 site-packages/torch/distributed/run.py:774] *****************************************\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:54: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  ALLREDUCE = partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:54: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  ALLREDUCE = partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:55: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  FP16_COMPRESS = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:55: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  FP16_COMPRESS = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:58: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  BF16_COMPRESS = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:58: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  BF16_COMPRESS = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:61: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  QUANTIZE_PER_TENSOR = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:61: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  QUANTIZE_PER_TENSOR = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:64: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  QUANTIZE_PER_CHANNEL = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:64: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  QUANTIZE_PER_CHANNEL = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:67: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  POWER_SGD = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:67: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  POWER_SGD = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:74: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  POWER_SGD_RANK2 = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:74: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  POWER_SGD_RANK2 = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:80: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  BATCHED_POWER_SGD = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:80: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  BATCHED_POWER_SGD = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:85: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  BATCHED_POWER_SGD_RANK2 = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:85: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  BATCHED_POWER_SGD_RANK2 = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:90: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  NOOP = partial(\n/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:90: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\n  NOOP = partial(\n  0%|                                                 | 0/12190 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n{'loss': 2.6379, 'grad_norm': 4.762500762939453, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.08}\n{'loss': 2.6222, 'grad_norm': 2.3519115447998047, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.16}\n{'loss': 2.6145, 'grad_norm': 2.249897003173828, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.25}\n{'loss': 2.5757, 'grad_norm': 1.5418566465377808, 'learning_rate': 3.99e-05, 'epoch': 0.33}\n{'loss': 2.5361, 'grad_norm': 2.070622205734253, 'learning_rate': 4.99e-05, 'epoch': 0.41}\n{'loss': 2.424, 'grad_norm': 1.300615906715393, 'learning_rate': 4.9576561163387515e-05, 'epoch': 0.49}\n{'loss': 2.1654, 'grad_norm': 1.8215751647949219, 'learning_rate': 4.9148845166809243e-05, 'epoch': 0.57}\n{'loss': 2.0119, 'grad_norm': 2.325028419494629, 'learning_rate': 4.8721129170230965e-05, 'epoch': 0.66}\n{'loss': 1.8704, 'grad_norm': 3.4572620391845703, 'learning_rate': 4.8293413173652694e-05, 'epoch': 0.74}\n{'loss': 1.636, 'grad_norm': 9.16623592376709, 'learning_rate': 4.786569717707442e-05, 'epoch': 0.82}\n{'loss': 1.4077, 'grad_norm': 4.913926601409912, 'learning_rate': 4.743798118049615e-05, 'epoch': 0.9}\n{'loss': 1.2245, 'grad_norm': 3.155449390411377, 'learning_rate': 4.701026518391788e-05, 'epoch': 0.98}\n 10%|███▌                                | 1219/12190 [14:03<2:18:41,  1.32it/s]\n  0%|                                                    | 0/16 [00:00<?, ?it/s]\u001b[A\n 12%|█████▌                                      | 2/16 [00:00<00:01, 10.38it/s]\u001b[A\n 25%|███████████                                 | 4/16 [00:00<00:02,  4.83it/s]\u001b[A\n 31%|█████████████▊                              | 5/16 [00:00<00:02,  5.18it/s]\u001b[A\n 38%|████████████████▌                           | 6/16 [00:01<00:01,  5.12it/s]\u001b[A\n 44%|███████████████████▎                        | 7/16 [00:01<00:01,  4.93it/s]\u001b[A\n 50%|██████████████████████                      | 8/16 [00:01<00:01,  4.47it/s]\u001b[A\n 56%|████████████████████████▊                   | 9/16 [00:01<00:01,  5.04it/s]\u001b[A\n 62%|██████████████████████████▉                | 10/16 [00:01<00:01,  5.05it/s]\u001b[A\n 69%|█████████████████████████████▌             | 11/16 [00:02<00:00,  5.15it/s]\u001b[A\n 75%|████████████████████████████████▎          | 12/16 [00:02<00:00,  5.43it/s]\u001b[A\n 81%|██████████████████████████████████▉        | 13/16 [00:02<00:00,  4.65it/s]\u001b[A\n 88%|█████████████████████████████████████▋     | 14/16 [00:02<00:00,  4.75it/s]\u001b[A\n 94%|████████████████████████████████████████▎  | 15/16 [00:02<00:00,  4.79it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.1730120182037354, 'eval_runtime': 3.6274, 'eval_samples_per_second': 275.683, 'eval_steps_per_second': 4.411, 'epoch': 1.0}\n 10%|███▌                                | 1219/12190 [14:07<2:18:41,  1.32it/s]\n100%|███████████████████████████████████████████| 16/16 [00:03<00:00,  4.56it/s]\u001b[A\n                                                                                \u001b[A/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n  warnings.warn(  # warn only once\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\n[rank0]:   File \"<frozen runpy>\", line 88, in _run_code\n[rank0]:   File \"/kaggle/working/KernelLTL/train.py\", line 121, in <module>\n[rank0]:     main()\n[rank0]:     ~~~~^^\n[rank0]:   File \"/kaggle/working/KernelLTL/train.py\", line 114, in main\n[rank0]:     trainer.train()\n[rank0]:     ~~~~~~~~~~~~~^^\n[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 2325, in train\n[rank0]:     return inner_training_loop(\n[rank0]:         args=args,\n[rank0]:     ...<2 lines>...\n[rank0]:         ignore_keys_for_eval=ignore_keys_for_eval,\n[rank0]:     )\n[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 2790, in _inner_training_loop\n[rank0]:     self._maybe_log_save_evaluate(\n[rank0]:     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n[rank0]:         tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate=learning_rate\n[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:     )\n[rank0]:     ^\n[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 3228, in _maybe_log_save_evaluate\n[rank0]:     self._save_checkpoint(model, trial)\n[rank0]:     ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 3325, in _save_checkpoint\n[rank0]:     self.save_model(output_dir, _internal_call=True)\n[rank0]:     ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 4227, in save_model\n[rank0]:     self._save(output_dir)\n[rank0]:     ~~~~~~~~~~^^^^^^^^^^^^\n[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 4336, in _save\n[rank0]:     self.processing_class.save_pretrained(output_dir)\n[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AttributeError: 'LTLTokenizer' object has no attribute 'save_pretrained'\n 10%|███▌                                | 1219/12190 [14:08<2:07:18,  1.44it/s]\n[rank0]:[W1012 19:11:36.593114510 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nW1012 19:11:38.109000 424 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 437 closing signal SIGTERM\nE1012 19:11:38.374000 424 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 436) of binary: /root/.local/share/mamba/envs/kernelltl/bin/python3.13\nTraceback (most recent call last):\n  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/bin/torchrun\"\u001b[0m, line \u001b[35m7\u001b[0m, in \u001b[35m<module>\u001b[0m\n    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m357\u001b[0m, in \u001b[35mwrapper\u001b[0m\n    return f(*args, **kwargs)\n  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m901\u001b[0m, in \u001b[35mmain\u001b[0m\n    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\n    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mrun\u001b[0m\n    \u001b[31melastic_launch(\u001b[0m\n    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n        \u001b[31mconfig=config,\u001b[0m\n        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\n        \u001b[31mentrypoint=cmd,\u001b[0m\n        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\n    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\n    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\n  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35m__call__\u001b[0m\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m277\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\n    raise ChildFailedError(\n    ...<2 lines>...\n    )\n\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\n============================================================\ntrain FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-10-12_19:11:38\n  host      : 45e03bb62d3b\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 436)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\u001b[0m\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import math\nimport os\nimport torch\n\n# Hyperparameters\nnum_epochs = 10\n\nlearning_rate = 5e-5\n\nT       = 20\nAP      = 5\nseed    = 1\n\neps     = 0.01\ndelta   = 1 - 0.99\nN       = math.ceil((2 / eps**2) * math.log(2 / delta))\n\nm       = 1024\n    \n# Create output directory\noutput_dir = \"ltl_model_outputs\"\nos.makedirs(output_dir, exist_ok=True)\nlocal_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\nif local_rank != -1 and torch.cuda.is_available():\n    torch.cuda.set_device(local_rank)\n# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n# torch.use_deterministic_algorithms(True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\nfrom tokenizer_class import LTLTokenizer\nfrom kernel_class import LTLKernel\nfrom dataset_class import LTLDataset\n\n# Initialize tokenizer (adjust n_ap based on your needs)\ntokenizer = LTLTokenizer(n_ap=AP)\n\n# Initialize kernel for semantic embeddings\nkernel = LTLKernel(T, AP, seed)  # adjust T and AP as needed\nkernel.sample_traces_kernel(N)  # adjust N based on your needs\nkernel.sample_anchor_formulas_kernel(m)  # m should match model's n_embd\nkernel.build_F()\n\n# Create datasets\ntrain_dataset = LTLDataset()\ntrain_dataset.construct_dataset_from_kernel(\n    kernel=kernel,\n    k=78000,  # adjust dataset size as needed\n    p_leaf=0.45,\n    max_depth=1000,\n    batch_size=10240\n)\n\neval_dataset = LTLDataset()\neval_dataset.construct_dataset_from_kernel(\n    kernel=kernel,\n    k=1000,  # smaller validation set\n    p_leaf=0.45,\n    max_depth=1000,\n    batch_size=10240\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from model_class import LTLModel\nfrom config_class import LTLConfig\nfrom training_utils import SemanticEvaluationCallback\n\n# Create model configuration and model\nconfig = LTLConfig(\n    n_embd=m  # must match kernel's anchor set size (m)\n)\n\nmodel = LTLModel(config, semantic_emb_dim=m)  # semantic_emb_dim must match kernel's anchor set size\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_epochs,\n    learning_rate=learning_rate,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=f\"{output_dir}/logs\",\n    logging_steps=100,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    dataloader_num_workers=4,\n    dataloader_pin_memory=True,\n    ddp_find_unused_parameters=False\n)\n\n# Initialize callback\nsemantic_callback = SemanticEvaluationCallback(\n    kernel=kernel,\n    tokenizer=tokenizer\n)\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=lambda batch : tokenizer.collate_batch(batch, model.config.n_positions),\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    processing_class=tokenizer,\n    callbacks=[semantic_callback]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train\ntrainer.train()\n\n# Save final model\ntrainer.save_model(os.path.join(output_dir, \"final_model\"))\ntokenizer.save_vocab(os.path.join(output_dir, \"vocab.json\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6549ca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:13:02.518932Z",
     "iopub.status.busy": "2025-10-13T16:13:02.518727Z",
     "iopub.status.idle": "2025-10-13T16:13:03.597743Z",
     "shell.execute_reply": "2025-10-13T16:13:03.597048Z"
    },
    "papermill": {
     "duration": 1.08326,
     "end_time": "2025-10-13T16:13:03.599077",
     "exception": false,
     "start_time": "2025-10-13T16:13:02.515817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'KernelLTL'...\r\n",
      "remote: Enumerating objects: 251, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (26/26), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\r\n",
      "remote: Total 251 (delta 8), reused 16 (delta 4), pack-reused 225 (from 1)\u001b[K\r\n",
      "Receiving objects: 100% (251/251), 109.33 KiB | 2.38 MiB/s, done.\r\n",
      "Resolving deltas: 100% (137/137), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/MeesLursen/KernelLTL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f387ff78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:13:03.605257Z",
     "iopub.status.busy": "2025-10-13T16:13:03.605033Z",
     "iopub.status.idle": "2025-10-13T16:13:03.953796Z",
     "shell.execute_reply": "2025-10-13T16:13:03.952821Z"
    },
    "papermill": {
     "duration": 0.352999,
     "end_time": "2025-10-13T16:13:03.955065",
     "exception": false,
     "start_time": "2025-10-13T16:13:03.602066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!cd /kaggle/working/KernelLTL && git pull && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c52527fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:13:03.960530Z",
     "iopub.status.busy": "2025-10-13T16:13:03.960285Z",
     "iopub.status.idle": "2025-10-13T16:13:03.963960Z",
     "shell.execute_reply": "2025-10-13T16:13:03.963411Z"
    },
    "papermill": {
     "duration": 0.007696,
     "end_time": "2025-10-13T16:13:03.965065",
     "exception": false,
     "start_time": "2025-10-13T16:13:03.957369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/working/KernelLTL\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5386df2",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-10-13T16:13:03.970436Z",
     "iopub.status.busy": "2025-10-13T16:13:03.970232Z",
     "iopub.status.idle": "2025-10-13T16:16:20.924212Z",
     "shell.execute_reply": "2025-10-13T16:16:20.923381Z"
    },
    "papermill": {
     "duration": 196.958553,
     "end_time": "2025-10-13T16:16:20.925684",
     "exception": false,
     "start_time": "2025-10-13T16:13:03.967131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin/micromamba\r\n",
      "\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\r\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\r\n",
      "conda-forge/linux-64 \u001b[33m━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\r\n",
      "conda-forge/noarch   \u001b[90m━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\r\n",
      "conda-forge/linux-64 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m 155.4kB /  47.8MB @ 813.5kB/s  0.1s\r\n",
      "conda-forge/noarch   ╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   1.3MB /  22.8MB @   6.6MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\r\n",
      "conda-forge/linux-64 ╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   3.5MB /  47.8MB @  12.0MB/s  0.2s\r\n",
      "conda-forge/noarch   ━╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m   2.8MB /  22.8MB @   9.6MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.4s\r\n",
      "conda-forge/linux-64 ━╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m   5.6MB /  47.8MB @  16.2MB/s  0.3s\r\n",
      "conda-forge/noarch   ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m   4.5MB /  22.8MB @  12.9MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\r\n",
      "conda-forge/linux-64 ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m  12.6MB /  47.8MB @  27.8MB/s  0.4s\r\n",
      "conda-forge/noarch   ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m   5.2MB /  22.8MB @  11.6MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\r\n",
      "conda-forge/linux-64 ━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━\u001b[0m  19.1MB /  47.8MB @  34.4MB/s  0.5s\r\n",
      "conda-forge/noarch   ━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━━\u001b[0m   9.4MB /  22.8MB @  16.8MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.7s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━\u001b[0m  25.8MB /  47.8MB @  39.2MB/s  0.6s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━\u001b[0m  11.3MB /  22.8MB @  17.2MB/s  0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━\u001b[0m  32.0MB /  47.8MB @  41.9MB/s  0.7s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━━\u001b[0m  15.2MB /  22.8MB @  19.9MB/s  0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━\u001b[0m  40.0MB /  47.8MB @  46.1MB/s  0.8s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━━\u001b[0m  16.4MB /  22.8MB @  19.0MB/s  0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  44.6MB /  47.8MB @  45.9MB/s  0.9s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m  17.9MB /  22.8MB @  18.4MB/s  0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  44.6MB /  47.8MB @  45.9MB/s  1.0s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m  17.9MB /  22.8MB @  18.4MB/s  1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\r\n",
      "conda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━\u001b[0m  44.6MB /  47.8MB @  45.9MB/s  1.1s\r\n",
      "conda-forge/noarch   ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m  17.9MB /  22.8MB @  18.4MB/s  1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                47.8MB @  45.9MB/s  1.2s\r\n",
      "[+] 1.3s\r\n",
      "conda-forge/noarch ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━\u001b[0m  18.7MB /  22.8MB @  14.7MB/s  1.2s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\r\n",
      "conda-forge/noarch ━━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━\u001b[0m  18.7MB /  22.8MB @  14.7MB/s  1.3s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                  22.8MB @  14.7MB/s  1.4s\r\n",
      "\u001b[?25h\r\n",
      "\r\n",
      "Transaction\r\n",
      "\r\n",
      "  Prefix: /root/.local/share/mamba/envs/kernelltl\r\n",
      "\r\n",
      "  Updating specs:\r\n",
      "\r\n",
      "   - python=3.13.7\r\n",
      "   - pip\r\n",
      "\r\n",
      "\r\n",
      "  Package               Version  Build               Channel          Size\r\n",
      "────────────────────────────────────────────────────────────────────────────\r\n",
      "  Install:\r\n",
      "────────────────────────────────────────────────────────────────────────────\r\n",
      "\r\n",
      "  \u001b[32m+ _libgcc_mutex   \u001b[0m        0.1  conda_forge         conda-forge       3kB\r\n",
      "  \u001b[32m+ _openmp_mutex   \u001b[0m        4.5  2_gnu               conda-forge      24kB\r\n",
      "  \u001b[32m+ bzip2           \u001b[0m      1.0.8  hda65f42_8          conda-forge     260kB\r\n",
      "  \u001b[32m+ ca-certificates \u001b[0m  2025.10.5  hbd8a1cb_0          conda-forge     156kB\r\n",
      "  \u001b[32m+ ld_impl_linux-64\u001b[0m       2.44  ha97dd6f_2          conda-forge     747kB\r\n",
      "  \u001b[32m+ libexpat        \u001b[0m      2.7.1  hecca717_0          conda-forge      75kB\r\n",
      "  \u001b[32m+ libffi          \u001b[0m      3.4.6  h2dba641_1          conda-forge      57kB\r\n",
      "  \u001b[32m+ libgcc          \u001b[0m     15.2.0  h767d61c_7          conda-forge     823kB\r\n",
      "  \u001b[32m+ libgomp         \u001b[0m     15.2.0  h767d61c_7          conda-forge     448kB\r\n",
      "  \u001b[32m+ liblzma         \u001b[0m      5.8.1  hb9d3cd8_2          conda-forge     113kB\r\n",
      "  \u001b[32m+ libmpdec        \u001b[0m      4.0.0  hb9d3cd8_0          conda-forge      91kB\r\n",
      "  \u001b[32m+ libsqlite       \u001b[0m     3.50.4  h0c1763c_0          conda-forge     933kB\r\n",
      "  \u001b[32m+ libuuid         \u001b[0m     2.41.2  he9a06e4_0          conda-forge      37kB\r\n",
      "  \u001b[32m+ libzlib         \u001b[0m      1.3.1  hb9d3cd8_2          conda-forge      61kB\r\n",
      "  \u001b[32m+ ncurses         \u001b[0m        6.5  h2d0b736_3          conda-forge     892kB\r\n",
      "  \u001b[32m+ openssl         \u001b[0m      3.5.4  h26f9b46_0          conda-forge       3MB\r\n",
      "  \u001b[32m+ pip             \u001b[0m       25.2  pyh145f28c_0        conda-forge       1MB\r\n",
      "  \u001b[32m+ python          \u001b[0m     3.13.7  h2b335a9_100_cp313  conda-forge      34MB\r\n",
      "  \u001b[32m+ python_abi      \u001b[0m       3.13  8_cp313             conda-forge       7kB\r\n",
      "  \u001b[32m+ readline        \u001b[0m        8.2  h8c095d6_2          conda-forge     282kB\r\n",
      "  \u001b[32m+ tk              \u001b[0m     8.6.13  noxft_hd72426e_102  conda-forge       3MB\r\n",
      "  \u001b[32m+ tzdata          \u001b[0m      2025b  h78e105d_0          conda-forge     123kB\r\n",
      "\r\n",
      "  Summary:\r\n",
      "\r\n",
      "  Install: 22 packages\r\n",
      "\r\n",
      "  Total download: 46MB\r\n",
      "\r\n",
      "────────────────────────────────────────────────────────────────────────────\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Transaction starting\r\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\r\n",
      "Downloading      ╸\u001b[33m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B                            0.0s\r\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\r\n",
      "Downloading  (5) \u001b[33m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B libsqlite                  0.0s\r\n",
      "Extracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibsqlite                                          932.6kB @   3.0MB/s  0.1s\r\n",
      "pip                                                  1.2MB @   1.5MB/s  0.1s\r\n",
      "openssl                                              3.1MB @   9.1MB/s  0.1s\r\n",
      "[+] 0.2s\r\n",
      "Downloading  (5) ━╸\u001b[33m━━━━━━━━━━━━━━━━━━━━━\u001b[0m   5.8MB ld_impl_linux-64           0.1s\r\n",
      "Extracting   (2) \u001b[90m━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━\u001b[0m       0 libsqlite                  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gtk                                                   3.3MB @  15.9MB/s  0.2s\r\n",
      "libgcc                                             822.6kB @  ??.?MB/s  0.1s\r\n",
      "ld_impl_linux-64                                   747.2kB @  ??.?MB/s  0.1s\r\n",
      "libgomp                                            447.9kB @  ??.?MB/s  0.1s\r\n",
      "[+] 0.3s\r\n",
      "Downloading  (5) ━━━━━━━━━╸\u001b[33m━━━━━━━━━━━━━\u001b[0m  20.3MB bzip2                      0.2s\r\n",
      "Extracting   (6) ╸\u001b[33m━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m       1 ld_impl_linux-64           0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gncurses                                            891.6kB @  ??.?MB/s  0.1s\r\n",
      "bzip2                                              260.3kB @  ??.?MB/s  0.1s\r\n",
      "readline                                           282.5kB @  ??.?MB/s  0.1s\r\n",
      "ca-certificates                                    155.9kB @  ??.?MB/s  0.1s\r\n",
      "tzdata                                             123.0kB @  ??.?MB/s  0.1s\r\n",
      "[+] 0.4s\r\n",
      "Downloading  (5) ━━━━━━━━━━━━╸\u001b[33m━━━━━━━━━━\u001b[0m  27.0MB libexpat                   0.3s\r\n",
      "Extracting   (7) ━━━━╸\u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m       5 ld_impl_linux-64           0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gliblzma                                            112.9kB @  ??.?MB/s  0.1s\r\n",
      "libexpat                                            74.8kB @  ??.?MB/s  0.1s\r\n",
      "libzlib                                             61.0kB @  ??.?MB/s  0.1s\r\n",
      "libffi                                              57.4kB @  ??.?MB/s  0.1s\r\n",
      "[+] 0.5s\r\n",
      "Downloading  (5) ━━━━━━━━━━━━━━━╸\u001b[33m━━━━━━━\u001b[0m  32.5MB _openmp_mutex              0.4s\r\n",
      "Extracting   (5) ━━━━━━━━━━╸\u001b[33m━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m      11 libexpat                   0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibmpdec                                            91.2kB @  ??.?MB/s  0.2s\r\n",
      "libuuid                                             37.1kB @  ??.?MB/s  0.1s\r\n",
      "python_abi                                           7.0kB @  ??.?MB/s  0.1s\r\n",
      "_openmp_mutex                                       23.6kB @  ??.?MB/s  0.1s\r\n",
      "[+] 0.6s\r\n",
      "Downloading  (2) ━━━━━━━━━━━━━━━━━━╸\u001b[33m━━━━\u001b[0m  39.7MB _libgcc_mutex              0.5s\r\n",
      "Extracting   (3) ━━━━━━━━━━━━━━━━╸\u001b[33m━━━╸\u001b[0m\u001b[90m━━\u001b[0m      17 libuuid                    0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G_libgcc_mutex                                        2.6kB @  ??.?MB/s  0.1s\r\n",
      "python                                              33.6MB @  47.6MB/s  0.6s\r\n",
      "[+] 0.7s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.6s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━╸\u001b[33m━╸\u001b[0m\u001b[90m━\u001b[0m      20 ncurses                    0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.6s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.6s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.6s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.6s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.6s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\r\n",
      "Downloading      ━━━━━━━━━━━━━━━━━━━━━━━  46.3MB                            0.6s\r\n",
      "Extracting   (1) ━━━━━━━━━━━━━━━━━━━━╸\u001b[33m━━\u001b[0m      21 python                     1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25hLinking ld_impl_linux-64-2.44-ha97dd6f_2\r\n",
      "Linking libgomp-15.2.0-h767d61c_7\r\n",
      "Linking _libgcc_mutex-0.1-conda_forge\r\n",
      "Linking _openmp_mutex-4.5-2_gnu\r\n",
      "Linking libgcc-15.2.0-h767d61c_7\r\n",
      "Linking libmpdec-4.0.0-hb9d3cd8_0\r\n",
      "Linking ncurses-6.5-h2d0b736_3\r\n",
      "Linking libzlib-1.3.1-hb9d3cd8_2\r\n",
      "Linking libuuid-2.41.2-he9a06e4_0\r\n",
      "Linking liblzma-5.8.1-hb9d3cd8_2\r\n",
      "Linking libffi-3.4.6-h2dba641_1\r\n",
      "Linking bzip2-1.0.8-hda65f42_8\r\n",
      "Linking libexpat-2.7.1-hecca717_0\r\n",
      "Linking readline-8.2-h8c095d6_2\r\n",
      "Linking libsqlite-3.50.4-h0c1763c_0\r\n",
      "Linking tk-8.6.13-noxft_hd72426e_102\r\n",
      "Linking python_abi-3.13-8_cp313\r\n",
      "Linking tzdata-2025b-h78e105d_0\r\n",
      "Linking ca-certificates-2025.10.5-hbd8a1cb_0\r\n",
      "Linking openssl-3.5.4-h26f9b46_0\r\n",
      "Linking python-3.13.7-h2b335a9_100_cp313\r\n",
      "Linking pip-25.2-pyh145f28c_0\r\n",
      "\r\n",
      "Transaction finished\r\n",
      "\r\n",
      "\r\n",
      "To activate this environment, use:\r\n",
      "\r\n",
      "    micromamba activate kernelltl\r\n",
      "\r\n",
      "Or to execute a single command in this environment, use:\r\n",
      "\r\n",
      "    micromamba run -n kernelltl mycommand\r\n",
      "\r\n",
      "Collecting accelerate==1.10.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 1))\r\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting certifi==2025.10.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 2))\r\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting charset-normalizer==3.4.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 3))\r\n",
      "  Downloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\r\n",
      "Collecting filelock==3.20.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 4))\r\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Collecting fsspec==2025.9.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 5))\r\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting hf-xet==1.1.10 (from -r /kaggle/working/KernelLTL/requirements.txt (line 6))\r\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\r\n",
      "Collecting huggingface-hub==0.35.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 7))\r\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting idna==3.10 (from -r /kaggle/working/KernelLTL/requirements.txt (line 8))\r\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting Jinja2==3.1.6 (from -r /kaggle/working/KernelLTL/requirements.txt (line 9))\r\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting MarkupSafe==3.0.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 10))\r\n",
      "  Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\r\n",
      "Collecting mpmath==1.3.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 11))\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Collecting networkx==3.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 12))\r\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting numpy==2.3.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 13))\r\n",
      "  Downloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 14))\r\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 15))\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 16))\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 17))\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from -r /kaggle/working/KernelLTL/requirements.txt (line 18))\r\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from -r /kaggle/working/KernelLTL/requirements.txt (line 19))\r\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 20))\r\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 21))\r\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 22))\r\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 23))\r\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 24))\r\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\r\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 25))\r\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from -r /kaggle/working/KernelLTL/requirements.txt (line 26))\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from -r /kaggle/working/KernelLTL/requirements.txt (line 27))\r\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\r\n",
      "Collecting packaging==25.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 28))\r\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting psutil==7.1.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 29))\r\n",
      "  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\r\n",
      "Collecting PyYAML==6.0.3 (from -r /kaggle/working/KernelLTL/requirements.txt (line 30))\r\n",
      "  Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\r\n",
      "Collecting regex==2025.9.18 (from -r /kaggle/working/KernelLTL/requirements.txt (line 31))\r\n",
      "  Downloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\r\n",
      "Collecting requests==2.32.5 (from -r /kaggle/working/KernelLTL/requirements.txt (line 32))\r\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\r\n",
      "Collecting safetensors==0.6.2 (from -r /kaggle/working/KernelLTL/requirements.txt (line 33))\r\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n",
      "Collecting setuptools==80.9.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 34))\r\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\r\n",
      "Collecting sympy==1.14.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 35))\r\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting tokenizers==0.22.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 36))\r\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\r\n",
      "Collecting torch==2.8.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 37))\r\n",
      "  Downloading torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (30 kB)\r\n",
      "Collecting tqdm==4.67.1 (from -r /kaggle/working/KernelLTL/requirements.txt (line 38))\r\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "Collecting transformers==4.57.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 39))\r\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\r\n",
      "Collecting triton==3.4.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 40))\r\n",
      "  Downloading triton-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\r\n",
      "Collecting typing_extensions==4.15.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 41))\r\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\r\n",
      "Collecting urllib3==2.5.0 (from -r /kaggle/working/KernelLTL/requirements.txt (line 42))\r\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\r\n",
      "Downloading numpy-2.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading certifi-2025.10.5-py3-none-any.whl (163 kB)\r\n",
      "Downloading charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\r\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\r\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\r\n",
      "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\r\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\r\n",
      "Downloading markupsafe-3.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\r\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\r\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\r\n",
      "Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\r\n",
      "Downloading pyyaml-6.0.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading regex-2025.9.18-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (802 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.1/802.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\r\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\r\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\r\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading torch-2.8.0-cp313-cp313-manylinux_2_28_x86_64.whl (887.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m  \u001b[33m0:00:17\u001b[0m\r\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\r\n",
      "\u001b[?25hDownloading triton-3.4.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\r\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\r\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, urllib3, typing_extensions, tqdm, sympy, setuptools, safetensors, regex, PyYAML, psutil, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, Jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers, accelerate\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/42\u001b[0m [accelerate]\r\n",
      "\u001b[1A\u001b[2KSuccessfully installed Jinja2-3.1.6 MarkupSafe-3.0.3 PyYAML-6.0.3 accelerate-1.10.1 certifi-2025.10.5 charset-normalizer-3.4.3 filelock-3.20.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.3 idna-3.10 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 packaging-25.0 psutil-7.1.0 regex-2025.9.18 requests-2.32.5 safetensors-0.6.2 setuptools-80.9.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.8.0 tqdm-4.67.1 transformers-4.57.0 triton-3.4.0 typing_extensions-4.15.0 urllib3-2.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!wget -qO- https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n",
    "!./bin/micromamba create -y -n kernelltl -c conda-forge python=3.13.7 pip\n",
    "!./bin/micromamba run -n kernelltl pip install -r /kaggle/working/KernelLTL/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05a19784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:16:21.053990Z",
     "iopub.status.busy": "2025-10-13T16:16:21.053345Z",
     "iopub.status.idle": "2025-10-13T16:37:08.692856Z",
     "shell.execute_reply": "2025-10-13T16:37:08.691908Z"
    },
    "papermill": {
     "duration": 1247.727943,
     "end_time": "2025-10-13T16:37:08.694436",
     "exception": false,
     "start_time": "2025-10-13T16:16:20.966493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1013 16:16:23.140000 198 site-packages/torch/distributed/run.py:774] \r\n",
      "W1013 16:16:23.140000 198 site-packages/torch/distributed/run.py:774] *****************************************\r\n",
      "W1013 16:16:23.140000 198 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W1013 16:16:23.140000 198 site-packages/torch/distributed/run.py:774] *****************************************\r\n",
      "Finished building Kernel.\r\n",
      "Finished building Kernel.\r\n",
      "Finished train and eval dataset construction.\r\n",
      "Finished train and eval dataset construction.\r\n",
      "Started training.\r\n",
      "Started training.\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:54: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  ALLREDUCE = partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:55: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  FP16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:58: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BF16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:61: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_TENSOR = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:64: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_CHANNEL = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:67: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:74: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:80: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:85: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:54: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  ALLREDUCE = partial(_ddp_comm_hook_wrapper, comm_hook=default.allreduce_hook)\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:90: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  NOOP = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:55: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  FP16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:58: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BF16_COMPRESS = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:61: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_TENSOR = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:64: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  QUANTIZE_PER_CHANNEL = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:67: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:74: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:80: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:85: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  BATCHED_POWER_SGD_RANK2 = partial(\r\n",
      "/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/algorithms/ddp_comm_hooks/__init__.py:90: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in enum.member() if you want to preserve the old behavior\r\n",
      "  NOOP = partial(\r\n",
      "  0%|                                                 | 0/12190 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\r\n",
      "{'loss': 2.6638, 'grad_norm': 4.466134548187256, 'learning_rate': 9.900000000000002e-06, 'epoch': 0.08}\r\n",
      "{'loss': 2.6236, 'grad_norm': 2.3024837970733643, 'learning_rate': 1.9900000000000003e-05, 'epoch': 0.16}\r\n",
      "{'loss': 2.6155, 'grad_norm': 2.351686954498291, 'learning_rate': 2.9900000000000002e-05, 'epoch': 0.25}\r\n",
      "{'loss': 2.5745, 'grad_norm': 1.680539846420288, 'learning_rate': 3.99e-05, 'epoch': 0.33}\r\n",
      "{'loss': 2.5321, 'grad_norm': 2.047318696975708, 'learning_rate': 4.99e-05, 'epoch': 0.41}\r\n",
      "{'loss': 2.4062, 'grad_norm': 1.3538707494735718, 'learning_rate': 4.9576561163387515e-05, 'epoch': 0.49}\r\n",
      "{'loss': 2.1103, 'grad_norm': 2.266030788421631, 'learning_rate': 4.9148845166809243e-05, 'epoch': 0.57}\r\n",
      "{'loss': 1.9432, 'grad_norm': 3.8181450366973877, 'learning_rate': 4.8721129170230965e-05, 'epoch': 0.66}\r\n",
      "{'loss': 1.6976, 'grad_norm': 4.9350266456604, 'learning_rate': 4.8293413173652694e-05, 'epoch': 0.74}\r\n",
      "{'loss': 1.4527, 'grad_norm': 6.857290744781494, 'learning_rate': 4.786569717707442e-05, 'epoch': 0.82}\r\n",
      "{'loss': 1.2869, 'grad_norm': 4.365206718444824, 'learning_rate': 4.743798118049615e-05, 'epoch': 0.9}\r\n",
      "{'loss': 1.1853, 'grad_norm': 3.1446917057037354, 'learning_rate': 4.701026518391788e-05, 'epoch': 0.98}\r\n",
      " 10%|███▌                                | 1219/12190 [13:47<2:15:40,  1.35it/s][rank0]: Traceback (most recent call last):\r\n",
      "[rank0]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n",
      "[rank0]:   File \"<frozen runpy>\", line 88, in _run_code\r\n",
      "[rank0]:   File \"/kaggle/working/KernelLTL/train.py\", line 132, in <module>\r\n",
      "[rank0]:     main()\r\n",
      "[rank0]:     ~~~~^^\r\n",
      "[rank0]:   File \"/kaggle/working/KernelLTL/train.py\", line 125, in main\r\n",
      "[rank0]:     trainer.train()\r\n",
      "[rank0]:     ~~~~~~~~~~~~~^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 2325, in train\r\n",
      "[rank0]:     return inner_training_loop(\r\n",
      "[rank0]:         args=args,\r\n",
      "[rank0]:     ...<2 lines>...\r\n",
      "[rank0]:         ignore_keys_for_eval=ignore_keys_for_eval,\r\n",
      "[rank0]:     )\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer.py\", line 2789, in _inner_training_loop\r\n",
      "[rank0]:     self.control = self.callback_handler.on_epoch_end(args, self.state, self.control)\r\n",
      "[rank0]:                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer_callback.py\", line 516, in on_epoch_end\r\n",
      "[rank0]:     return self.call_event(\"on_epoch_end\", args, state, control)\r\n",
      "[rank0]:            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/transformers/trainer_callback.py\", line 556, in call_event\r\n",
      "[rank0]:     result = getattr(callback, event)(\r\n",
      "[rank0]:         args,\r\n",
      "[rank0]:     ...<8 lines>...\r\n",
      "[rank0]:         **kwargs,\r\n",
      "[rank0]:     )\r\n",
      "[rank0]:   File \"/kaggle/working/KernelLTL/training_utils.py\", line 64, in on_epoch_end\r\n",
      "[rank0]:     for batch in eval_dataloader:\r\n",
      "[rank0]:                  ^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 734, in __next__\r\n",
      "[rank0]:     data = self._next_data()\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1516, in _next_data\r\n",
      "[rank0]:     return self._process_data(data, worker_id)\r\n",
      "[rank0]:            ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/dataloader.py\", line 1551, in _process_data\r\n",
      "[rank0]:     data.reraise()\r\n",
      "[rank0]:     ~~~~~~~~~~~~^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/_utils.py\", line 769, in reraise\r\n",
      "[rank0]:     raise exception\r\n",
      "[rank0]: TypeError: Caught TypeError in DataLoader worker process 0.\r\n",
      "[rank0]: Original Traceback (most recent call last):\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\r\n",
      "[rank0]:     data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\r\n",
      "[rank0]:     return self.collate_fn(data)\r\n",
      "[rank0]:            ~~~~~~~~~~~~~~~^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\r\n",
      "[rank0]:     return collate(batch, collate_fn_map=default_collate_fn_map)\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 212, in collate\r\n",
      "[rank0]:     collate(samples, collate_fn_map=collate_fn_map)\r\n",
      "[rank0]:     ~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "[rank0]:   File \"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\r\n",
      "[rank0]:     raise TypeError(default_collate_err_msg_format.format(elem_type))\r\n",
      "[rank0]: TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'formula_class.And'>\r\n",
      "\r\n",
      "W1013 16:37:07.938000 198 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 211 closing signal SIGTERM\r\n",
      "E1013 16:37:08.255000 198 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: -11) local_rank: 0 (pid: 210) of binary: /root/.local/share/mamba/envs/kernelltl/bin/python3.13\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/bin/torchrun\"\u001b[0m, line \u001b[35m7\u001b[0m, in \u001b[35m<module>\u001b[0m\r\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\r\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\"\u001b[0m, line \u001b[35m357\u001b[0m, in \u001b[35mwrapper\u001b[0m\r\n",
      "    return f(*args, **kwargs)\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m901\u001b[0m, in \u001b[35mmain\u001b[0m\r\n",
      "    \u001b[31mrun\u001b[0m\u001b[1;31m(args)\u001b[0m\r\n",
      "    \u001b[31m~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/run.py\"\u001b[0m, line \u001b[35m892\u001b[0m, in \u001b[35mrun\u001b[0m\r\n",
      "    \u001b[31melastic_launch(\u001b[0m\r\n",
      "    \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\r\n",
      "        \u001b[31mconfig=config,\u001b[0m\r\n",
      "        \u001b[31m~~~~~~~~~~~~~~\u001b[0m\r\n",
      "        \u001b[31mentrypoint=cmd,\u001b[0m\r\n",
      "        \u001b[31m~~~~~~~~~~~~~~~\u001b[0m\r\n",
      "    \u001b[31m)\u001b[0m\u001b[1;31m(*cmd_args)\u001b[0m\r\n",
      "    \u001b[31m~\u001b[0m\u001b[1;31m^^^^^^^^^^^\u001b[0m\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35m__call__\u001b[0m\r\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\r\n",
      "  File \u001b[35m\"/root/.local/share/mamba/envs/kernelltl/lib/python3.13/site-packages/torch/distributed/launcher/api.py\"\u001b[0m, line \u001b[35m277\u001b[0m, in \u001b[35mlaunch_agent\u001b[0m\r\n",
      "    raise ChildFailedError(\r\n",
      "    ...<2 lines>...\r\n",
      "    )\r\n",
      "\u001b[1;35mtorch.distributed.elastic.multiprocessing.errors.ChildFailedError\u001b[0m: \u001b[35m\r\n",
      "=====================================================\r\n",
      "train FAILED\r\n",
      "-----------------------------------------------------\r\n",
      "Failures:\r\n",
      "  <NO_OTHER_FAILURES>\r\n",
      "-----------------------------------------------------\r\n",
      "Root Cause (first observed failure):\r\n",
      "[0]:\r\n",
      "  time      : 2025-10-13_16:37:07\r\n",
      "  host      : 71f3138c5624\r\n",
      "  rank      : 0 (local_rank: 0)\r\n",
      "  exitcode  : -11 (pid: 210)\r\n",
      "  error_file: <N/A>\r\n",
      "  traceback : Signal 11 (SIGSEGV) received by PID 210\r\n",
      "=====================================================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!cd /kaggle/working/KernelLTL && \\\n",
    " PYTHONPATH=$PWD /kaggle/working/bin/micromamba run -n kernelltl \\\n",
    " torchrun --nproc_per_node=2 -m train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5066f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T16:37:08.918144Z",
     "iopub.status.busy": "2025-10-13T16:37:08.917307Z",
     "iopub.status.idle": "2025-10-13T16:37:09.953786Z",
     "shell.execute_reply": "2025-10-13T16:37:09.953050Z"
    },
    "papermill": {
     "duration": 1.124599,
     "end_time": "2025-10-13T16:37:09.955074",
     "exception": false,
     "start_time": "2025-10-13T16:37:08.830475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ref\": \"\", \"url\": \"https://www.kaggle.com/\", \"status\": \"Error\", \"error\": \"Please upload at least one file\", \"invalidTags\": []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, shutil\n",
    "\n",
    "import os, shutil\n",
    "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
    "shutil.copy(\"/kaggle/input/kaggle-api-token/kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
    "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
    "\n",
    "from kaggle import api\n",
    "ds_dir = \"/kaggle/working/KernelLTL/ltl_model_outputs\"\n",
    "os.makedirs(ds_dir, exist_ok=True)\n",
    "\n",
    "metadata = {\n",
    "    \"title\": \"LTL model output\",\n",
    "    \"id\": \"tayhnd5sgdtjevyreuw/kernel-ltl-model-outputs\",\n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "}\n",
    "with open(os.path.join(ds_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "api.dataset_create_new(ds_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aea698",
   "metadata": {
    "papermill": {
     "duration": 0.086593,
     "end_time": "2025-10-13T16:37:10.129672",
     "exception": false,
     "start_time": "2025-10-13T16:37:10.043079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "import shutil, os\n",
    "\n",
    "def kaggle_export(output_dir, zip_name):\n",
    "    \"\"\"Create a zip file and export your work in the kaggle working directory.\n",
    "    Args:\n",
    "        output_dir (str): The path to the directory to be zipped.\n",
    "        zip_name (str): The name of the zip file to be created.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"[Error]: output directory '{output_dir}' does not exist\")\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            shutil.make_archive(zip_name, 'zip', output_dir)\n",
    "            print(f\"[Success]: Your zip file is successfully created!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Error]: An error occurred while creating zip: {e}\")\n",
    "\n",
    "kaggle_export(\"/kaggle/working/KernelLTL/ltl_model_outputs\", \"model_outputs\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6e9153",
   "metadata": {
    "papermill": {
     "duration": 0.086336,
     "end_time": "2025-10-13T16:37:10.301129",
     "exception": false,
     "start_time": "2025-10-13T16:37:10.214793",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "\n",
    "learning_rate = 5e-5\n",
    "\n",
    "T       = 20\n",
    "AP      = 5\n",
    "seed    = 1\n",
    "\n",
    "eps     = 0.01\n",
    "delta   = 1 - 0.99\n",
    "N       = math.ceil((2 / eps**2) * math.log(2 / delta))\n",
    "\n",
    "m       = 1024\n",
    "    \n",
    "# Create output directory\n",
    "output_dir = \"ltl_model_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "if local_rank != -1 and torch.cuda.is_available():\n",
    "    torch.cuda.set_device(local_rank)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b499c",
   "metadata": {
    "papermill": {
     "duration": 0.086759,
     "end_time": "2025-10-13T16:37:10.473368",
     "exception": false,
     "start_time": "2025-10-13T16:37:10.386609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from tokenizer_class import LTLTokenizer\n",
    "from kernel_class import LTLKernel\n",
    "from dataset_class import LTLDataset\n",
    "\n",
    "# Initialize tokenizer (adjust n_ap based on your needs)\n",
    "tokenizer = LTLTokenizer(n_ap=AP)\n",
    "\n",
    "# Initialize kernel for semantic embeddings\n",
    "kernel = LTLKernel(T, AP, seed)  # adjust T and AP as needed\n",
    "kernel.sample_traces_kernel(N)  # adjust N based on your needs\n",
    "kernel.sample_anchor_formulas_kernel(m)  # m should match model's n_embd\n",
    "kernel.build_F()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LTLDataset()\n",
    "train_dataset.construct_dataset_from_kernel(\n",
    "    kernel=kernel,\n",
    "    k=78000,  # adjust dataset size as needed\n",
    "    p_leaf=0.45,\n",
    "    max_depth=1000,\n",
    "    batch_size=10240\n",
    ")\n",
    "\n",
    "eval_dataset = LTLDataset()\n",
    "eval_dataset.construct_dataset_from_kernel(\n",
    "    kernel=kernel,\n",
    "    k=1000,  # smaller validation set\n",
    "    p_leaf=0.45,\n",
    "    max_depth=1000,\n",
    "    batch_size=10240\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275a374",
   "metadata": {
    "papermill": {
     "duration": 0.086093,
     "end_time": "2025-10-13T16:37:10.645609",
     "exception": false,
     "start_time": "2025-10-13T16:37:10.559516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "from model_class import LTLModel\n",
    "from config_class import LTLConfig\n",
    "from training_utils import SemanticEvaluationCallback\n",
    "\n",
    "# Create model configuration and model\n",
    "config = LTLConfig(\n",
    "    n_embd=m  # must match kernel's anchor set size (m)\n",
    ")\n",
    "\n",
    "model = LTLModel(config, semantic_emb_dim=m)  # semantic_emb_dim must match kernel's anchor set size\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    ddp_find_unused_parameters=False\n",
    ")\n",
    "\n",
    "# Initialize callback\n",
    "semantic_callback = SemanticEvaluationCallback(\n",
    "    kernel=kernel,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=lambda batch : tokenizer.collate_batch(batch, model.config.n_positions),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[semantic_callback]\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c1c30",
   "metadata": {
    "papermill": {
     "duration": 0.087152,
     "end_time": "2025-10-13T16:37:10.817994",
     "exception": false,
     "start_time": "2025-10-13T16:37:10.730842",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "'''\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
    "tokenizer.save_vocab(os.path.join(output_dir, \"vocab.json\"))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8481667,
     "sourceId": 13369742,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1454.390483,
   "end_time": "2025-10-13T16:37:11.323855",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-13T16:12:56.933372",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
